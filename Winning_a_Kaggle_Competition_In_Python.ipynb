{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Winning_a_Kaggle_Competition_In_Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXqSeFDyEi+0dNpwbR+xss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amir-ab/DATACAMP_ML_Scientist/blob/main/Winning_a_Kaggle_Competition_In_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2v4wta96U1G"
      },
      "source": [
        "# Part 1 : Kaggle competitions process\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXp9YHJt47r_"
      },
      "source": [
        "### Explore train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77s6RD0HnNhf"
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read train data\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# Look at the shape of the data\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "# Look at the head() of the data\n",
        "print(train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOUotZ605CNd"
      },
      "source": [
        "### Explore test data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyalX7J_5GqS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the test data\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Print train and test columns\n",
        "print('Train columns:', train.columns.tolist())\n",
        "print('Test columns:', test.columns.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQZoSCjz5Pl9"
      },
      "source": [
        "### Train a simple model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp5-QBCS5Ro9"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Read the train data\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# Create a Random Forest object\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Train a model\n",
        "rf.fit(X=train[['store', 'item']], y=train['sales'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0tGGogp5Tzl"
      },
      "source": [
        "### Prepare a submission\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAlVrh5h5X8N"
      },
      "source": [
        "# Read test and sample submission data\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Show the head() of the sample_submission\n",
        "print(sample_submission.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFSxz0kU5f-l"
      },
      "source": [
        "####the metric is Mean Squared Error (the lower its value the better). Train and validation metrics for all the models are presented in the table below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQjkEkQ5Zn2"
      },
      "source": [
        "### Train XGBoost models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or76sbkn5qK1"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create DMatrix on train data\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
        "                     label=train['sales'])\n",
        "\n",
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 2,\n",
        "          'silent': 1}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdiBjHpS5uyl"
      },
      "source": [
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 8,\n",
        "          'silent': 1}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlScvWmz5xAd"
      },
      "source": [
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 15,\n",
        "          'silent': 1}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_15 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdyOUtgI57OV"
      },
      "source": [
        "### Explore overfitting XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2eG58I954Te"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
        "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
        "\n",
        "# For each of 3 trained models\n",
        "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(dtrain)     \n",
        "    test_pred = model.predict(dtest)          \n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse_train = mean_squared_error(train['sales'], train_pred)                  \n",
        "    mse_test = mean_squared_error(test['sales'], test_pred)\n",
        "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTuT6pnG6bkP"
      },
      "source": [
        "# Part 2 : Dive into the Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n_mXliigfHq"
      },
      "source": [
        "### Define a competition metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyckcVISggMB"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import MSE from sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define your own MSE function\n",
        "def own_mse(y_true, y_pred):\n",
        "  \t# Raise differences to the power of 2\n",
        "    squares = np.power(y_true - y_pred, 2)\n",
        "    # Find mean over all observations\n",
        "    err = np.mean(squares)\n",
        "    return err\n",
        "\n",
        "print('Sklearn MSE: {:.5f}. '.format(mean_squared_error(y_regression_true, y_regression_pred)))\n",
        "print('Your MSE: {:.5f}. '.format(own_mse(y_regression_true, y_regression_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdq1PjI3gjQy"
      },
      "source": [
        "### EDA statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Dx1qGxgo_6"
      },
      "source": [
        "# Shapes of train and test data\n",
        "print('Train shape:', train.shape)\n",
        "print('Test shape:', test.shape)\n",
        "\n",
        "# Train head()\n",
        "print(train.head())\n",
        "\n",
        "# Describe the target variable\n",
        "print(train.fare_amount.describe())\n",
        "\n",
        "# Train distribution of passengers within rides\n",
        "print(train.passenger_count.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roGCV7XLgyLa"
      },
      "source": [
        "### EDA plots I\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03PJULVCgtPJ"
      },
      "source": [
        "# Calculate the ride distance\n",
        "train['distance_km'] = haversine_distance(train)\n",
        "\n",
        "# Draw a scatterplot\n",
        "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
        "plt.xlabel('Fare amount')\n",
        "plt.ylabel('Distance, km')\n",
        "plt.title('Fare amount based on the distance')\n",
        "\n",
        "# Limit on the distance\n",
        "plt.ylim(0, 50)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uClOACYSg8c6"
      },
      "source": [
        "### EDA plots II\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcN-KUgvg7nB"
      },
      "source": [
        "# Create hour feature\n",
        "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
        "train['hour'] = train.pickup_datetime.dt.hour\n",
        "\n",
        "# Find median fare_amount for each hour\n",
        "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
        "\n",
        "# Plot the line plot\n",
        "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
        "plt.xlabel('Hour of the day')\n",
        "plt.ylabel('Median fare amount')\n",
        "plt.title('Fare amount based on day time')\n",
        "plt.xticks(range(24))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnNcZ2v4g_sJ"
      },
      "source": [
        "### K-fold cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ipkqToehKHB"
      },
      "source": [
        "# Import KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in kf.split(train):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8rUh55RhMYr"
      },
      "source": [
        "### Stratified K-fold\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ijiZVjhchZ"
      },
      "source": [
        "# Import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp6nSV0dhgSp"
      },
      "source": [
        "### Time K-fold\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRyh0WphkfB"
      },
      "source": [
        "# Create TimeSeriesSplit object\n",
        "time_kfold = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Iterate through each split\n",
        "fold = 0\n",
        "for train_index, test_index in time_kfold.split(train):\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    \n",
        "    print('Fold :', fold)\n",
        "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
        "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYUeBG_xhk75"
      },
      "source": [
        "### Overall validation score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwA3Ust6hqVp"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Initialize 3-fold time cross-validation\n",
        "kf = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Get MSE scores for each cross-validation split\n",
        "mse_scores = get_fold_mse(train, kf)\n",
        "\n",
        "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umzmnX_OhsUx"
      },
      "source": [
        "# Part 3 : Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7jri5DC4JM-"
      },
      "source": [
        "### Arithmetical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjdMJzoo4RMf"
      },
      "source": [
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
        "\n",
        "# Look at the updated RMSE\n",
        "print('RMSE with total area:', get_kfold_rmse(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjHcscq4TFO"
      },
      "source": [
        "### Date features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXd3fMmn4XOe"
      },
      "source": [
        "# Concatenate train and test together\n",
        "taxi = pd.concat([train, test])\n",
        "\n",
        "# Convert pickup date to datetime object\n",
        "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
        "\n",
        "# Create a day of week feature\n",
        "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
        "\n",
        "# Create an hour feature\n",
        "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
        "\n",
        "# Split back into train and test\n",
        "new_train = taxi[taxi['id'].isin(train['id'])]\n",
        "new_test = taxi[taxi['id'].isin(test['id'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy-yu_Zn4cFG"
      },
      "source": [
        "### Label encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHCMTGS04k4G"
      },
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Create new features\n",
        "houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])\n",
        "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
        "\n",
        "# Look at new features\n",
        "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPuAdN-F42is"
      },
      "source": [
        "### One-Hot encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQFUSs7M46O6"
      },
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Look at feature distributions\n",
        "print(houses['RoofStyle'].value_counts(), '\\n')\n",
        "print(houses['CentralAir'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EoRVpc5Bct"
      },
      "source": [
        "### Mean target encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAuhI33U5Cm1"
      },
      "source": [
        "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "    # Calculate global mean on the train data\n",
        "    global_mean = train[target].mean()\n",
        "    \n",
        "    # Group by the categorical feature and calculate its properties\n",
        "    train_groups = train.groupby(categorical)\n",
        "    category_sum = train_groups[target].sum()\n",
        "    category_size = train_groups.size()\n",
        "    \n",
        "    # Calculate smoothed mean target statistics\n",
        "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
        "    \n",
        "    # Apply statistics to the test data and fill new categories\n",
        "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
        "    return test_feature.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXoCGfo5Egd"
      },
      "source": [
        "### K-fold cross-validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGqfe_hS5LMv"
      },
      "source": [
        "# Create 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
        "\n",
        "# For each folds split\n",
        "for train_index, test_index in kf.split(bryant_shots):\n",
        "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
        "\n",
        "    # Create mean target encoded feature\n",
        "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
        "                                                                           test=cv_test,\n",
        "                                                                           target='shot_made_flag',\n",
        "                                                                           categorical='game_id',\n",
        "                                                                           alpha=5)\n",
        "    # Look at the encoding\n",
        "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oApRJ6AN5Nvl"
      },
      "source": [
        "### Beyond binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_hFZ-_b5RrF"
      },
      "source": [
        "# Create mean target encoded feature\n",
        "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
        "                                                                     test=test,\n",
        "                                                                     target='SalePrice',\n",
        "                                                                     categorical='RoofStyle',\n",
        "                                                                     alpha=10)\n",
        "\n",
        "# Look at the encoding\n",
        "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7I1_eEz5Tmt"
      },
      "source": [
        "### Find missing data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w11sPUGx5YYF"
      },
      "source": [
        "# Read dataframe\n",
        "twosigma = pd.read_csv('twosigma_train.csv')\n",
        "\n",
        "# Find the number of missing values in each column\n",
        "print(twosigma.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-rBi9So5aw-"
      },
      "source": [
        "### Impute missing data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipaue2Jx5erl"
      },
      "source": [
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create mean imputer\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Price imputation\n",
        "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sXPy50R4xNE"
      },
      "source": [
        "#Part 4 : Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfEhITkW6AFV"
      },
      "source": [
        "### Replicate validation score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SbLMSN5XSxj"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Calculate the mean fare_amount on the validation_train data\n",
        "naive_prediction = np.mean(validation_train['fare_amount'])\n",
        "\n",
        "# Assign naive prediction to all the holdout observations\n",
        "validation_test['pred'] = naive_prediction\n",
        "\n",
        "# Measure the local RMSE\n",
        "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
        "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjFJ00nXVYx"
      },
      "source": [
        "### Baseline based on the date\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8njA_mAEXa7y"
      },
      "source": [
        "# Get pickup hour from the pickup_datetime column\n",
        "train['hour'] = train['pickup_datetime'].dt.hour\n",
        "test['hour'] = test['pickup_datetime'].dt.hour\n",
        "\n",
        "# Calculate average fare_amount grouped by pickup hour \n",
        "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
        "\n",
        "# Make predictions on the test set\n",
        "test['fare_amount'] = test.hour.map(hour_groups)\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6lrDfSvXclc"
      },
      "source": [
        "### Baseline based on the gradient boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goAcCOnUXht-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Select only numeric features\n",
        "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
        "            'dropoff_latitude', 'passenger_count', 'hour']\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['fare_amount'] = rf.predict(test[features])\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCMSrT_BXlQk"
      },
      "source": [
        "### Grid search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWKPQJmbXrB9"
      },
      "source": [
        "# Possible max depth values\n",
        "max_depth_grid = [3,6,9,12,15]\n",
        "results = {}\n",
        "\n",
        "# For each value in the grid\n",
        "for max_depth_candidate in max_depth_grid:\n",
        "    # Specify parameters for the model\n",
        "    params = {'max_depth': max_depth_candidate}\n",
        "\n",
        "    # Calculate validation score for a particular hyperparameter\n",
        "    validation_score = get_cv_score(train, params)\n",
        "\n",
        "    # Save the results for each max depth value\n",
        "    results[max_depth_candidate] = validation_score   \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_jcZkbeXtvb"
      },
      "source": [
        "### 2D grid search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wyrg14BXzXh"
      },
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grids\n",
        "max_depth_grid = [3,5,7]\n",
        "subsample_grid = [0.8,0.9,1.0]\n",
        "results = {}\n",
        "\n",
        "# For each couple in the grid\n",
        "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
        "    params = {'max_depth': max_depth_candidate,\n",
        "              'subsample': subsample_candidate}\n",
        "    validation_score = get_cv_score(train, params)\n",
        "    # Save the results for each couple\n",
        "    results[(max_depth_candidate, subsample_candidate)] = validation_score   \n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_bMwDRFX1Bn"
      },
      "source": [
        "### Model blending\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7dihLyrX7L5"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])\n",
        "\n",
        "# Find mean of model predictions\n",
        "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
        "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xc9MVpkX9Es"
      },
      "source": [
        "### Model stacking I\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOxk9Z1cYACa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Split train data into two parts\n",
        "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
        "\n",
        "# Train a Gradient Boosting model on Part 1\n",
        "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
        "\n",
        "# Train a Random Forest model on Part 1\n",
        "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3i-8xdFYB4T"
      },
      "source": [
        "### Model stacking II\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlpNZDiAYE_U"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create linear regression model without the intercept\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# Train 2nd level model on the Part 2 data\n",
        "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
        "\n",
        "# Make stacking predictions on the test data\n",
        "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
        "\n",
        "# Look at the model coefficients\n",
        "print(lr.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdgiG_1KYGuz"
      },
      "source": [
        "### Testing Kaggle forum ideas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5o9S31WYLbW"
      },
      "source": [
        "# Drop passenger_count column\n",
        "new_train_1 = train.drop('passenger_count', axis=1)\n",
        "\n",
        "# Compare validation scores\n",
        "initial_score = get_cv_score(train)\n",
        "new_score = get_cv_score(new_train_1)\n",
        "\n",
        "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hylk2VTXs10"
      },
      "source": [
        "# Create copy of the initial train DataFrame\n",
        "new_train_2 = train.copy()\n",
        "\n",
        "# Find sum of pickup latitude and ride distance\n",
        "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
        "\n",
        "# Compare validation scores\n",
        "initial_score = get_cv_score(train)\n",
        "new_score = get_cv_score(new_train_2)\n",
        "\n",
        "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}